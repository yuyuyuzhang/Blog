# 六、内存管理

[[_TOC_]]

## 1. 存储器分级策略

### (1) 为什么会有存储器分级策略

#### ① 我们的需求是什么

我们希望存储器`速度快、体积小、空间大、能耗低、断电数据不丢失`

#### ② 需求的实现制约

现实中我们往往无法实现所有需求

* 存储器体积小 => 存储控件收到制约
* 存储器电子元件密度很大 => 散热会有问题
* 存储器距离 CPU 远 => 传输会有延迟，传输速度下降
  * 信号是以`光速`传输的，宏观世界里光速很快，但在计算机世界里光速并没有想象的那么快，时钟信号为 1GHz 的 CPU，时钟信号的一个周期为 1/10 亿秒，光速为 3 亿米每秒，因此一个时钟周期内，光只能前进 30 厘米，因此即使存储器距离 CPU 稍微远了一点，运行速度也会下降非常明显
  * 为什么不将内存放到 CPU：CPU 整个电路的散热和体积会出现问题，并且服务器无法再定制内存，也就是说 CPU 在出场时就决定了其内存大小，如果想更换更大的内存就必须更换 CPU，而`组装定制化`是非常重要的诉求

#### ③ 需求的分级实现

既然无法用一块存储器实现所有的需求，那就将需求`分级实现`

根据`数据的使用频率`使用不同的存储器，高频使用的数据，使用最贵的材料放到距离 CPU 最近的位置，读写速度最快，低频使用的数据，使用便宜的材料放到距离 CPU 较远的位置，读写速度较慢

### (2) 存储器分级策略

通常将存储器分成以下几个级别

#### ① 寄存器

* 寄存器紧挨 CPU 的控制单元和逻辑运算单元
* 寄存器主要用于暂存 `CPU 执行计算机程序时的操作数据和中间结果`，寄存器具有与 CPU 相同的速度，完全能与 CPU 协调工作

#### ② L1-Cache

* L1-Cache 位于 CPU，相比寄存器位置距离 CPU 核心更远，但造价更低，通常 L1-Cache 大小在几十 Kb 到几百 Kb 不等，读写速度在 `2~4` 个 CPU 时钟周期
* L1-Cache 主要用于存放 `CPU 将要运行或刚运行过的程序和数据`，也就是备份内存中 CPU 常用的数据，减少 CPU 对内存的访问次数，用于`解决 CPU 和内存间的速度匹配问题`

#### ③ L2-Cache

* L2-Cache 位于 CPU，相比 L1-Cache 位置距离 CPU 核心更远，但造价更低，大小比 L1-Cache 更大但具体大小要看 CPU 型号，读写速度在 `10~20` 个 CPU 周期
* L2-Cache 主要用于存放 `CPU 将要运行或刚运行过的程序和数据`，也就是备份内存中 CPU 常用的数据，减少 CPU 对内存的访问次数，用于`解决 CPU 和内存间的速度匹配问题`

#### ④ L3-Cache

* L3-Cache 位于 CPU，相比 L2-Cache 位置距离 CPU 核心更远，但造价更低，大小比 L2-Cache 更大但具体大小要看 CPU 型号，读写速度在 `20~60` 个 CPU 周期
* L3-Cache 主要用于存放 `CPU 将要运行或刚运行过的程序和数据`，也就是备份内存中 CPU 常用的数据，减少 CPU 对内存的访问次数，用于`解决 CPU 和内存间的速度匹配问题`

#### ⑤ 内存

* 内存的主要材料是`半导体硅`，插在主板上工作，距离 CPU 有一段距离，因此需要用`总线`和 CPU 连接
* 内存有了独立的空间，因此体积更大，造价也比上面的缓存更低，个人电脑的内存通常是 16G，有些服务器内存可以到几个 T
* 内存的读写速度大概在 `200~300` 个 CPU 周期
* 内存主要用于存放`运行中的程序和数据`，CPU 能够直接访问内存

#### ⑥ 磁盘、固态硬盘 SSD

* 磁盘、固态硬盘 SSD 的结构和内存类似，优点在于`断电后数据不丢失`，寄存器、缓存、内存断点后数据就丢失了
* 磁盘主要用于存放`未运行的程序和数据`，磁盘的信息必须`调入内存`后才能被 CPU 使用

![存储器分级策略](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E5%AD%98%E5%82%A8%E5%99%A8%E5%88%86%E7%BA%A7%E7%AD%96%E7%95%A5.png)

### (3) 内存数据查找

CPU 需要`内存`中某个数据时，按照以下顺序查找，最后没有才从`内存`中拿

![数据查找](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E6%95%B0%E6%8D%AE%E6%9F%A5%E6%89%BE.png)

### (4) 缓存条目结构

#### ① 线性存储器

缓存和内存都是线性存储器，也就是`数据一个个挨着存储`，如果把内存想象成一个只有`一列`的表格，那么缓存就是一个`多列`的表格，表格的每一行就是一个缓存条目

#### ② 2 列缓存条目

缓存本质上是存储 key-value 键值对，key - 内存地址，value - 对应内存地址中的值，因此我们可以将一个缓存条目设计成 2 列，CPU 读取到一个内存地址，就增加一个缓存条目，当 CPU 需要内存中某个数据时，先遍历 L1-Cache 每个缓存条目中的内存地址是否与查询的内存地址一致，一致就取出对应缓存条目中的值

这个方法需要遍历 L1-Cache 中的每个缓存条目，因此速度非常慢，最坏的情况下需要检查所有的缓存条目，因此该方案不可行

#### ③ 哈希函数映射缓存条目

我们需要一个更好的方法，让我们看到内存地址，就知道其在哪个缓存条目

例如有 1000 个内存地址编号为 0~999，10 个缓存条目编号为 0~9，每个内存编号用`数学方法`映射到一个缓存条目，例如内存地址 701 整除 10 得到缓存条目 1，这就构成了一个简单的`哈希函数：地址 % 10`，知道缓存条目后，再比较第一列内存地址和查询的内存地址是否相同

### (5) L1-Cache 指令预读

#### ① 为什么需要指令预读

* CPU 顺序执行内存中的指令，CPU 执行指令的速度一般在 `2~6` 个 CPU 时钟周期
* 由于存储器分级策略，内存的读写速度一般在 `200~300` 个 CPU 时钟周期
* 因此 CPU 不能从内存中一条条读取指令再执行，否则执行每条指令都需要 200~300 个 CPU 时钟周期

#### ② 指令预读的实现

* CPU 将内存中的指令预读几十上百条到 `L1-Cache`，因为 L1-Cache 的读写速度一般在 `2~4` 个时钟周期，可以跟的上 CPU 的指令执行速度
* 如果指令和数据都缓存在 L1-Cache，当数据缓存覆盖了指令缓存，就会造成很严重的后果，因此 L1-Cache 通常会分成`指令区`和`数据区`
* L2-Cache、L3-Cache 无需协助处理指令预读的问题，因此无需分成指令区和数据区

### (6) 缓存命中率

#### ① 命中

在缓存中找到需要的数据

#### ② 穿透

未从缓存中找到需要的数据

#### ③ 缓存命中率

* L1-Cache 的命中率在 `80%`，L1-Cache、L2-Cache、L3-Cache 加起来的命中率在 `95%`
* CPU 缓存的设计相当合理，95% 能名字缓存，只有 5% 会穿透到内存
* 正是由于缓存保证了很高的命中率，所以程序语言逐渐取消了让程序员操作寄存器的语法，多余的优化意义不大，而且容易出错

### (7) 缓存置换问题

L1-Cache 的缓存条目已满，CPU 又读取了内存，需要增加一个缓存条目到 L1-Cache，那么这个时候就需要一个算法计算 L1-Cache 中哪个旧的缓存条目被置换出去，这就是缓存置换问题

## 2. 虚拟内存

### (1) 虚拟内存

虚拟内存技术是为了解决内存不够用的问题，为什么内存不够用？因为程序越来越复杂

用户看到应用程序能在操作系统中正常运行，认为计算机的内存容量比应用程序大，也就是说用户感受到的内存容量比实际内存容量大的多，但用户看到的大容量只是一种错觉，这就是虚拟内存

虚拟内存是指具有`请求调入和置换功能`，能从逻辑上对内存容量加以扩充的一种方式，虚拟内存的逻辑容量由`内存容量和磁盘容量之和`决定

![虚拟内存](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98.png)

### (2) 页和页表

操作系统将虚拟内存分块成一个个`页 Page`，真实内存分块成一个个 `Frame`，操作系统管理`页表`，也就是`Page 到 Frame 之间的映射`

* Page 和 Frame 大小通常`相等`
* 页表记录某个 Page 对应的 Frame 编号，操作系统在内存中划分出小块区域维护页表
* 应用程序使用内存以`页`为单位，整齐的页能够`避免内存碎片问题`

![页和页表](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E9%A1%B5%E5%92%8C%E9%A1%B5%E8%A1%A8.png)

### (3) 页表条目

* 在 Absent：0 代表该页内容位于磁盘（触发缺页中断，去磁盘读取数据），1 代表该页内容位于内存
* 保护 Protection：代表该页内容用于读、写、还是执行
* 访问 Reference：代表该页内容是否被`读取`过
* 脏 Dirty：代表该页内容是否被`写入`过
* 缓存 Caching：代表该页是否可以被 CPU 缓存
* Frame Number：该页对应的 Frame 编号

![页表条目](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E9%A1%B5%E8%A1%A8%E6%9D%A1%E7%9B%AE.png)

### (4) 内存管理单元 MMU

当 CPU 需要执行一条指令时，如果指令设计到内存读写操作，CPU 会将虚拟地址给内存管理单元 MMU，MMU `自动`完成虚拟地址到物理地址的计算，然后 MMU 连接地址总线帮助 CPU 操作真实地址，因此 MMU 承担了`虚拟地址到物理地址的转换`以及 `CPU 对内存的操作`这两件事情

![MMU](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/MMU.png)

### (5) 转置检测缓冲区 TLB

指令的执行速度非常快，MMU 还需要`从内存中查询页表`，最快的内存查询也需要从 `CPU 缓存`中读取，例如读到 L2 缓存，每次操作需要几个 CPU 周期，也因此内存操作最好能在非常短的时间内完成，特别是 Page 到 Frame 的映射，最好不到 0.2 个 CPU 周期，这样就不会因为地址换算而增加指令执行的 CPU 周期

MMU 中有一个微型设备是转置检测缓冲区 TLB，TLB 中最主要的信息就是 Page 到 Frame 的映射，TLB 最简单的表达就是一个二维表格，每行是一个 Page 和 Frame，这样的一行称为缓存条目，TLB 的作用就是根据输入的 Page 找到 Frame，TLB 是`硬件`实现的，因此速度非常快

| Page Number | Frame Number |
| ----------- | ------------ |
|             |              |

#### TLB 失效（Miss）

* **软失效**
  * TLB 中没有找到输入的 Page，Frame 还在`内存`中
  * 这时需要`刷新 TLB`，如果 TLB 已满就需要选择一个已存在的缓存条目进行覆盖，这被称为`缓存置换`，缓存置换时通常希望高频使用的数据保留，低频使用的数据被置换
* **硬失效**
  * TLB 中没有找到输入的 Page，Frame 需要从`磁盘`加载
  * 这时首先需要操作系统触发一个`缺页中断`，原有需要读取内存的线程休眠，中断响应程序开始从磁盘读取对应的 Frame 到内存，读取完成后再次触发中断，通知更新 TLB 并且唤醒休眠的线程排队

### (6) 缓存置换算法

设计缓存置换算法的期望是每次将`未来使用频率最低`的数据置换出去

* **命中 Hit**：在缓存中找到数据
* **穿透 Miss**：没有在缓存中找到数据
* **缓存平均响应时间**：缓存访问时间 L + 穿透概率 M * 穿透后获取数据的平均时间 T
  * 缓存访问时间 L 通常是不变的，这和使用的缓存类型相关
  * 优秀的缓存置换算法可以将穿透概率 M 降到最低，也就可以让缓存平均响应时间降到最低

#### ① 最近未使用 NRU

最近未使用 NRU 就是优先把`最近没有使用`的数据置换出去，从概率上说，最近没有使用的数据未来使用的概率会比最近经常使用的数据低

页表条目的访问位 Reference：代表该页内容是否被读取过，脏位 Dirty 代表该页内容是否被写入过，这两种都被认为是访问过，一旦该页被访问，就用`硬件`将 Reference 置 1，然后设置一个定时器指定时间后将 Reference 清 0，一旦该页被写入过，就用硬件将 Dirty 置 1，然后设置一个定时器指定时间后将该页`写回磁盘`并将 Dirty 清 0，这种方式就构成了一个最基本的 NRU 算法，每次置换时，操作系统尽量选择 `Reference、Dirty 都是 0` 的页，而一个页如果在内存中停留太久没有新的读写，Reference、Dirty 都会被置 0，就可能会被置换

* **优点**：简单有效性能好，能够提高命中率，NRU 是一个被广泛使用的缓存置换算法
* **缺点**：只考虑了最近使用的情况，没有充分考虑综合的访问情况，对缓存命中率的提升有效

#### ② 最近最少使用 LRU

最近最少使用 LRU 就是优先将`最近最久没有使用`的数据置换出去，LRU 认为最近一段时间最少使用到的数据应该被淘汰，把空间让给最近频繁使用的数据，这样即便数据都被使用过，还是会根据使用频次多少进行淘汰

LRU 的常见实现方式是`双向链表`，用双向链表维护缓存条目，如果链表中某个缓存条目被使用过，就将其移动到表头，如果需要置换缓存条目，就直接从链表尾部删除

* **优点**：对缓存命中率的提升很大
* **缺点**：不适用于高性能常见，例如需要维护一个很大的链表来存储所有页，经常需要删除大量的页（缓存置换），并把大量的页移动到链表头部，这对于高性能场景来说是不可接受的

![LRU](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/LRU.png)

### (7) 虚拟地址到物理地址的换算

虚拟地址到物理地址的换算步骤如下

* 通过虚拟内存地址计算 Page 编号和页表条目编号
* 通过 Page 编号查询页表找到 Frame 编号
* 将虚拟内存地址换算成物理内存地址

例如程序要访问虚拟内存地址 100000，页大小为 4K

* Page 编号：100000 / 4*1024 = 24 余 1619，页编号 24，偏移量 1619（页表条目编号）
* Frame 编号：查询页表，得到编号 10
* 物理地址：4*1024 * 10 + 1619 = 42579

![虚拟地址到物理地址的换算](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80%E5%88%B0%E7%89%A9%E7%90%86%E5%9C%B0%E5%9D%80%E7%9A%84%E6%8D%A2%E7%AE%97.png)

### (8) 多级页表

假设有一个应用程序，初始化后需要 12M 内存，操作系统页大小为 4K，如何设计？

假设这个应用程序只有 3 个段，正文段（程序）、数据段（常量、全局变量）、堆栈段，一开始为 3 个段都分配 4M 内存，随着程序的运行，堆栈段的空间会继续增加，上不封顶

![大页面问题](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E5%A4%A7%E9%A1%B5%E9%9D%A2%E9%97%AE%E9%A2%98.png)

进程内部需要一个页表存储进程数据，如果进程内存上不封顶，页表设计成多少个条目合适呢？如果一个条目 4M，1024 个条目就可以支持 1024*4K = 4M 空间，如果进程需要 1G 空间，就需要 256K 个条目，预先为进程分配如此多的条目成本太高

为了减少条目的创建，可以使用多级页表，例如进程内部使用 4M 页表，操作系统使用 4K 页表，这就形成了二级页表，这个模型下进程如果需要 1G 空间，也只需要 1024 个条目

![二级页表](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E4%BA%8C%E7%BA%A7%E9%A1%B5%E8%A1%A8.png)

## 3. 内存回收

### (1) 内存泄漏

编程语言层面已经提供了垃圾回收机制，那为什么还会产生内存泄漏？

因为`应用程序的内存管理`和`应用程序的执行`，一直处于一种`并发`的状态，如果应用程序申请内存的速度超过内存回收的速度，内存就会被存满，当内存存满，操作系统就会频繁进行磁盘读写，所以我们观察到的系统性能下降，往往是一种突然的崩溃，因为一旦内存被占满，系统性能就开始雪崩式下降

### (2) 垃圾回收器 GC

编程语言提供的垃圾回收器 GC 往往是应用程序的实际内存管理者，一方面 GC 要承接操作系统虚拟内存的架构，另一方面 GC 还要为应用程序提供内存管理，GC 需要承担的具体工作如下

* 应用程序向 GC 申请内存
* GC 和操作系统交互，负责申请内存和释放内存
* GC 负责垃圾回收，标记不用的对象并回收
* GC 针对应用程序特性进行动态优化

![GC](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/GC.png)

#### 编程语言实现 GC 需关注的指标

* **吞吐量 Throughput**：应用程序线程执行用时占总用时的比例，例如 GC 吞吐量 99% 意味着 100s 的程序执行时间中应用程序线程运行 99s，GC 线程运行 1s
  * 如果单纯让 GC 尽快把工作做完，需要提升吞吐量，最直观的方法就是利用`多核优势`
  * 通常设计 GC 希望能支持`并行`处理任务，因为 GC 本身有着繁重的工作量，需要扫描所有对象，对内存进行标记清除等等
  * 但是如果 GC 算法并发度非常高，和单线程算法相比也会带来更多的其他开销，例如拆分任务的开销、解决同步问题的开销、空间开销等等，理想情况下当然是高并发较好，但考虑到计算本身的成本，需要一个折中的方案
* **足迹 FootPrint**：应用程序对内存的占用情况，例如应用程序运行需要 2G 内存，但是好的 GC 算法能够帮助减少 500M 的内存使用，满足足迹这个指标
  * 足迹就是指的 GC 的`空间开销`
* **暂停时间 Pause Time**：GC 执行时通常需要暂停应用程序执行避免同步问题，不同应用程序对某次内存回收可以暂停的时间需求是不同的，例如游戏暂停几毫秒用户都可能会有很大意见，网页暂停几秒用户都没有感知
  * GC 不能拥有太长的暂停时间，因为 GC 和应用程序是`并发`执行，如果 GC 导致应用程序暂停太久，对有的应用例如游戏来说是灾难性的
  * 如果暂停时间只允许很短，GC 和应用程序的交替就会非常频繁，这对 GC 算法的要求就会上升，因为每次应用程序执行后，都会产生新的变化，甚至对已有的 GC 结果产生影响

### (3) 引用计数算法

实现 GC 的最简单方案是引用计数算法

例如下图某个节点的引用计数是 2，代表有 2 个节点引用了它，如果一个节点的引用计数是 0，意味着没有任何一个节点引用它，此时这个节点理论上应该被回收，GC 不断扫描引用计数为 0 的节点进行回收，这就构成了一个最简单的内存回收算法

![引用计数算法](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E5%BC%95%E7%94%A8%E8%AE%A1%E6%95%B0%E7%AE%97%E6%B3%95.png)

#### 引用计数算法的缺陷

* **出错概率大**：循环引用问题，例如下图三个节点，因为循环引用，引用计数都是 1，就算这三个节点不会再被其他节点引用，GC 也不会回收它们

  ![循环引用问题](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E5%BE%AA%E7%8E%AF%E5%BC%95%E7%94%A8%E9%97%AE%E9%A2%98.png)

* **容错能力差**：`多线程环境`下引用计数算法一旦算错一次，例如没有处理好竞争条件，就无法再更正了，因为处理竞争条件本身比较耗费性能，因此一旦计算出错就会导致内存永远无法被回收

### (4) 跟踪算法

#### ① 跟踪 Root Tracing

跟踪原理：从引用路径上，如果一个对象的引用链中包含一个根对象，这个对象就是`活动`的，根对象是所有引用关系的源头

* 在跟踪过程中，如果一个对象和根对象间有连通关系，也就是从根对象开始遍历可以找到这个对象，代表有其他对象在引用这个对象，那么这个对象就不需要被回收
* 如果一个对象从根对象不可达，代表没有其他对象在引用这个对象，那么这个对象就需要被回收，即便这个对象存在`循环引用`，例如下图三个红色对象彼此循环引用，但是到根集合没有引用链，也会被回收，这就解决了循环引用问题

![跟踪](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E8%B7%9F%E8%B8%AA.png)

跟踪的优势如下，解决了引用计数算法的缺陷

* **出错概率小**：解决了循环引用问题
* **容错能力好**：在跟踪过程中，如果有一些本来应该回收的对象没有被计算出，例如并发，也不会导致这些对象永久无法回收，因为 GC 可以在下次执行跟踪时找到这些对象，这次计算出错下次是可以恢复的

#### ② 双色标记清除算法 Mark Sweep

标记清除算法的实现如下

* **白色**：需要回收的节点
* **黑色**：不需要回收的节点
* **标记 Mark**：一开始所有对象都是白色，从`根对象`开始执行深度优先遍历/广度优先遍历，将遍历到的所有对象染成黑色
* **清除 Sweep**：遍历`所有对象的集合`，将白色对象回收

如果 GC 程序在某个时刻暂停，然后开始执行用户程序，用户程序删除了某个已标记为黑色的对象的所有引用，由于用户程序无法通知 GC 程序，这个对象就会变成`浮动垃圾`，需要等待下一次 GC 程序执行时回收，如果用户程序和 GC 程序`交替执行`，用户程序不断修改，GC 程序不断执行标记清除算法，这中间就会产生大量浮动垃圾影响 GC 的效果

* Mark：找到不用的内存，Sweep：回收不用的内存，Mutation：执行应用程序
* 这三种任务不允许`并行`，对于 Mark、Sweep、Mutation 来说内存是共享的，如果并行执行需要同时处理大量竞争手段，这会增加非常多的开销
* 通常的 GC 程序都是 Mark/Sweep/Mutation 只能`交替执行`，一种任务执行的时候，另一种任务必须停止

![Mark&Sweep&Mutation](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/Mark%26Sweep%26Mutation.png)

#### ③ 三色标记清除算法

三色标记清除算法的实现如下

* **白色**：需要回收的节点
* **黑色**：不需要回收的节点
* **灰色**：还未完成标记的节点
* **标记 Mark**
  * 三色标记清除算法需要维护三个集合：白色集合、黑色集合、灰色集合，三个集合`互斥`，对象只能位于一个集合中
  * 一开始所有对象都放入白色集合

    ![三色标记清除算法1](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E4%B8%89%E8%89%B2%E6%A0%87%E8%AE%B0%E6%B8%85%E9%99%A4%E7%AE%97%E6%B3%951.png)

  * 将根节点能直接引用的对象放入灰色集合

    ![三色标记清除算法2](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E4%B8%89%E8%89%B2%E6%A0%87%E8%AE%B0%E6%B8%85%E9%99%A4%E7%AE%97%E6%B3%952.png)

  * 遍历灰色集合，将灰色对象引用的对象从白色集合放入灰色集合，然后将这个灰色对象放入黑色集合，重复该步骤直到灰色集合为空

    ![三色标记清除算法3](https://github.com/yuyuyuzhang/Blog/blob/master/images/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E4%B8%89%E8%89%B2%E6%A0%87%E8%AE%B0%E6%B8%85%E9%99%A4%E7%AE%97%E6%B3%953.png)
* **清除 Sweep**
  * 回收白色集合

## 4. 内存分类

当我们在电脑上双击打开 QQ 时，通过鼠标向 CPU 发送一条指令，CPU 接收到指令后将 QQ 程序从磁盘加载到内存，加载完成后 CPU 开始执行 QQ 程序，执行完成后 CPU 让 QQ 程序显示在显示器上，也就是用户看到的最终结果

CPU 在内存中运行应用程序时，需要内存空间存放数据，操作系统会划分出两种不同的内容空间用于存放数据，一种是堆，一种是栈

### (1) 进程和线程

* **进程**：进程是一个`可拥有资源`的基本单位
  * **进程实体**：进程控制块 PCB、程序代码、相关的数据
  * **进程资源**：进程可拥有的资源指的是计算机系统资源，主要是指 内存、IO 设备 ( 磁盘、输入设备、输出设备 )
* **线程**：线程是一个`可独立调度`的基本单位 ( 从进程的 2 个基本属性中分离出的 )
  8 **线程实体**：线程控制块 TCB、程序代码，相关的数据

### (2) 堆内存（Heap）

* 每个`进程`分配一个堆，堆是该进程内所有线程共享的
* 堆的大小不确定，需要的话可以不断增加
* 堆是没有结构的，数据可以任意存放

### (3) 栈内存（Stack）

* 每个`线程`分配一个栈，栈是每个线程独占的
* 栈的大小确定，数据超过这个大小，就会发生栈溢出
* 栈是有结构的，数据按照一定次序存放，可以明确知道每个数据的大小
